{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PocaFoldAS inference (demo test data)\n",
    "Run the trained PocaFoldAS on the bundled demo test split (no simulation). Configure the checkpoint and output folder, then run inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project root\n",
    "Ensure the notebook runs from the repository root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "for candidate in [pathlib.Path.cwd(), *pathlib.Path.cwd().parents]:\n",
    "    if (candidate / 'setup.py').exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError('Could not locate the repository root. Please run this notebook from within the smlm project.')\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f'Working directory set to: {PROJECT_ROOT}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paths, checkpoint, and output location\n",
    "Default to demo dataset, tetrahedron checkpoint, output directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff10283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "CONFIG_PATH = pathlib.Path('configs/config_demo_data.yaml')  # model hyperparams\n",
    "CKPT_PATH = pathlib.Path(os.getenv('POCAFOLDAS_CKPT', 'weights/tetra.pth'))  # set to your trained weights\n",
    "DATA_ROOT = pathlib.Path(os.getenv('DEMO_TEST_ROOT', 'demo_data/tetrahedron_seed1234_test'))\n",
    "CLASSES = ['tetra']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def choose_output_dir():\n",
    "    env_path = os.getenv('POCAFOLDAS_INFER_OUT')\n",
    "    if env_path:\n",
    "        return pathlib.Path(env_path)\n",
    "    container_default = pathlib.Path('/workspace/smlm_inference')\n",
    "    if container_default.parent.exists() and os.access(container_default.parent, os.W_OK):\n",
    "        return container_default\n",
    "    return pathlib.Path('output/notebook_inference')\n",
    "\n",
    "OUTPUT_DIR = choose_output_dir()\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f'Config file not found: {CONFIG_PATH}')\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f'Data root not found: {DATA_ROOT}')\n",
    "if not CKPT_PATH.exists():\n",
    "    raise FileNotFoundError(f'Checkpoint not found: {CKPT_PATH}. Set POCAFOLDAS_CKPT to your trained weights.')\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Device:', DEVICE)\n",
    "print('Data root:', DATA_ROOT.resolve())\n",
    "print('Output dir:', OUTPUT_DIR.resolve())\n",
    "print('Checkpoint:', CKPT_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container note\n",
    "- Default output dir: `/workspace/smlm_inference`.\n",
    "- Override with `PocaFoldAS_INFER_OUT` or bind-mount a host folder, e.g. `-v /host/logs:/workspace/smlm_inference`.\n",
    "- Set `PocaFoldAS_CKPT` to a checkpoint inside the container or to a mounted path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load config hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(CONFIG_PATH.read_text())\n",
    "train_cfg = cfg.get('train', {})\n",
    "dset_cfg = cfg.get('dataset', {})\n",
    "\n",
    "num_dense = train_cfg.get('num_dense', 2048)\n",
    "latent_dim = train_cfg.get('latent_dim', 1024)\n",
    "grid_size = train_cfg.get('grid_size', 2)\n",
    "channels = train_cfg.get('channels', 3)\n",
    "classifier = bool(train_cfg.get('classifier', False))\n",
    "suffix = dset_cfg.get('suffix', '.csv')\n",
    "remove_part_prob = dset_cfg.get('remove_part_prob', 0.0)\n",
    "remove_outliers = dset_cfg.get('remove_outliers', False)\n",
    "remove_corners = dset_cfg.get('remove_corners', False)\n",
    "number_corners_remove = dset_cfg.get('number_corners_remove', [0, 1, 2])\n",
    "\n",
    "print(f\"Model: num_dense={num_dense}, latent_dim={latent_dim}, grid_size={grid_size}, channels={channels}, classifier={classifier}\")\n",
    "print(f\"Data cfg: suffix={suffix}, remove_part_prob={remove_part_prob}, remove_corners={remove_corners}, remove_outliers={remove_outliers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and dataloader\n",
    "Uses the demo test split (paired iso/aniso) and pads to the max point count found in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from model_architectures.transforms import Padding, ToTensor\n",
    "from helpers.data import get_highest_shape\n",
    "from dataset.Dataset import PairedAnisoIsoDataset\n",
    "\n",
    "highest_shape = get_highest_shape(str(DATA_ROOT), CLASSES, subfolders=['iso', 'aniso'], suffix=suffix)\n",
    "print('Highest shape:', highest_shape)\n",
    "\n",
    "pc_transforms = transforms.Compose([Padding(highest_shape), ToTensor()])\n",
    "\n",
    "dataset = PairedAnisoIsoDataset(\n",
    "    root_folder=str(DATA_ROOT),\n",
    "    suffix=suffix,\n",
    "    transform=pc_transforms,\n",
    "    classes_to_use=CLASSES,\n",
    "    remove_part_prob=remove_part_prob,\n",
    "    remove_outliers=remove_outliers,\n",
    "    remove_corners=remove_corners,\n",
    "    number_corners_to_remove=number_corners_remove,\n",
    ")\n",
    "\n",
    "print('Samples in dataset:', len(dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load model and checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_architectures.pocafoldas import PocaFoldAS\n",
    "\n",
    "model = PocaFoldAS(num_dense=num_dense, latent_dim=latent_dim, grid_size=grid_size, classifier=classifier, channels=channels)\n",
    "state = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "current = model.state_dict()\n",
    "filtered = {k: v for k, v in state.items() if k in current and v.size() == current[k].size()}\n",
    "current.update(filtered)\n",
    "model.load_state_dict(current, strict=False)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print('Loaded checkpoint with', len(filtered), 'matched keys (strict=False for flexibility).')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run inference and export a few predictions\n",
    "Saves the first few outputs as PLY files; collects Chamfer L1 for a quick sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from torch.utils.data import DataLoader\n",
    "from model_architectures.losses import l1_cd\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "ply_dir = OUTPUT_DIR / 'ply'\n",
    "ply_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_export = 5\n",
    "metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(loader):\n",
    "        partial = batch['partial_pc'].to(DEVICE).permute(0, 2, 1)\n",
    "        gt = batch['pc'].to(DEVICE)\n",
    "        filenames = batch['filename']\n",
    "\n",
    "        coarse, fine, *_ = model(partial)\n",
    "        pred = fine  # shape [B, N, 3]\n",
    "        l1 = l1_cd(pred, gt).item()\n",
    "        metrics.append(l1)\n",
    "\n",
    "        pred_np = pred.cpu().numpy()[0]\n",
    "        if idx < n_export:\n",
    "            out_path = ply_dir / f\"{idx}_{pathlib.Path(filenames[0]).stem}.ply\"\n",
    "            pc = o3d.geometry.PointCloud()\n",
    "            pc.points = o3d.utility.Vector3dVector(pred_np)\n",
    "            o3d.io.write_point_cloud(str(out_path), pc, write_ascii=True)\n",
    "\n",
    "print(f\"Exported {min(n_export, len(loader))} predictions to {ply_dir.resolve()}\")\n",
    "if metrics:\n",
    "    print(f\"Mean Chamfer L1 (demo subset): {np.mean(metrics):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}